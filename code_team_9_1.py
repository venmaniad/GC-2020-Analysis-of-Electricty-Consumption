# -*- coding: utf-8 -*-
"""Code_Team_9-1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1cUFc8lBGonIJ5ZhHHo007Ekbn1baz065
"""

# Commented out IPython magic to ensure Python compatibility.
import warnings
warnings.filterwarnings('ignore')
from datetime import datetime
import numpy as np
import pandas as pd
from scipy import stats
import statsmodels.api as sm
import matplotlib.pyplot as plt
from fbprophet import Prophet
import logging
import warnings
warnings.filterwarnings('ignore')
from sklearn.model_selection import TimeSeriesSplit
from sklearn import metrics
from sklearn.metrics import mean_squared_error
from statsmodels.tsa.holtwinters import ExponentialSmoothing
# %matplotlib inline

def make_comparison_dataframe(historical, forecast):
    """Join the history with the forecast.
    
       The resulting dataset will contain columns 'yhat', 'yhat_lower', 'yhat_upper' and 'y'.
    """
    return forecast.set_index('ds')[['yhat', 'yhat_lower', 'yhat_upper']].join(historical.set_index('ds'))

def calculate_forecast_errors(df, prediction_size):
    """Calculate MAPE and MAE of the forecast.
    
       Args:
           df: joined dataset with 'y' and 'yhat' columns.
           prediction_size: number of days at the end to predict.
    """
    
    # Make a copy
    df = df.copy()
    
    # Now we calculate the values of e_i and p_i according to the formulas given in the article above.
    df['e'] = df['y'] - df['yhat']
    df['p'] = 100 * df['e'] / df['y']
    rmse = np.sqrt(((df['y'] -df['yhat'])**2).sum()/len(df))
    # Recall that we held out the values of the last `prediction_size` days
    # in order to predict them and measure the quality of the model. 
    
    # Now cut out the part of the data which we made our prediction for.
    predicted_part = df[-prediction_size:]
    
    # Define the function that averages absolute error values over the predicted part.
    error_mean = lambda error_name: np.mean(np.abs(predicted_part[error_name]))
    
    # Now we can calculate MAPE and MAE and return the resulting dictionary of errors.
    return {'MAPE': error_mean('p'), 'MAE': error_mean('e'), 'RMSE': rmse}

df = pd.read_csv('../input/gc/train.csv')



# evaluation metric for 1 array

def myeval(pred, true):
    pred = np.array(pred)
    true = np.array(true)
    k = np.log(2)/(np.log(2.716)*100)

    su = 0
    for i in range(len(pred)):
        su+= ((pred[i]-true[i])**2)*np.exp(k*(int(i/96)+1))
    su = np.sqrt(su)/np.mean(true)
    return su

from xgboost import XGBRegressor





# fast fourier transform
data = pd.read_csv('../input/gc/train.csv')
main_meter = data[data['building_number']==1]['main_meter'][:]
main_meter
import matplotlib.pyplot as plt 
from matplotlib.pyplot import figure
figure(num=None, figsize=(15, 15), dpi=80, facecolor='w', edgecolor='k')
f = np.fft.fft(main_meter)
f_mod = np.abs(f)
plt.plot(range(100, 500), f_mod[100:500])
plt.show()
np.argmax(f_mod[10:])



df = pd.read_csv('../input/gc/train.csv')
for_test = pd.read_csv('../input/gc/train.csv')
data = pd.read_csv('../input/gc/train.csv')
buildings = [1, 2, 3, 4, 5]
meters = ['main_meter', 'sub_meter_1', 'sub_meter_2']
for_test['main_meter'] = 0
for_test['sub_meter_1'] = 0
for_test['sub_meter_2'] = 0
for_test.loc[:, meters]=0

evals=[]

for building in buildings:
    
    ev=0
    for meter in meters:
        
        #selecting building and meter
        
        print("building:")
        print(building)
        print("meter")
        print(meter)
        print("the value of rmse is")
        expreds = pd.DataFrame(columns=[])
        exgt = pd.DataFrame(columns=[])
        propreds = pd.DataFrame(columns=[])
        progt = pd.DataFrame(columns=[])
        
        
        # splitting data in train and validation
        
        dfx = data[data['building_number']==building][meter][:20000]
        df_test = for_test[for_test['building_number']==building][meter][20000:]
        
        df_prop = data[data['building_number']==building][:20000]
        df_prop = df_prop.loc[:, ['timestamp', meter]]
        
        df_prop.columns = ['ds', 'y']
        test_df_prop = for_test[for_test['building_number']==building][20000:]
        test_df_prop = test_df_prop.loc[:, ['timestamp', meter]]
        test_df_prop.columns = ['ds', 'y']
        
        # creating regressors
        date = pd.DatetimeIndex(df_prop['ds'])
        date.weekday
        df_prop['weekday'] = date.weekday
        # df_prop.drop('weekday', axis=1, inplace=True)
        df_prop['weekday'] = df_prop['weekday'].replace(0, 10)
        df_prop['weekday'] = df_prop['weekday'].replace(5, 0)
        df_prop['weekday'] = df_prop['weekday'].replace(4, 0)
        df_prop['weekday'] = df_prop['weekday'].replace(3, 0)
        df_prop['weekday'] = df_prop['weekday'].replace(2, 0)
        df_prop['weekday'] = df_prop['weekday'].replace(1, 0)
        df_prop['weekday'] = df_prop['weekday'].replace(6, 0)
        df_prop['weekday'] = df_prop['weekday'].replace(10, 10)

        df_prop['hr'] = date.hour
        df_prop

        date = pd.DatetimeIndex(test_df_prop['ds'])
        date.weekday
        test_df_prop['weekday'] = date.weekday
        # test_df_prop.drop('weekday', axis=1, inplace=True)
        test_df_prop['weekday'] = test_df_prop['weekday'].replace(0, 10)
        test_df_prop['weekday'] = test_df_prop['weekday'].replace(5, 0)
        test_df_prop['weekday'] = test_df_prop['weekday'].replace(4, 0)
        test_df_prop['weekday'] = test_df_prop['weekday'].replace(3, 0)
        test_df_prop['weekday'] = test_df_prop['weekday'].replace(2, 0)
        test_df_prop['weekday'] = test_df_prop['weekday'].replace(1, 0)
        test_df_prop['weekday'] = test_df_prop['weekday'].replace(6, 0)
        test_df_prop['weekday'] = test_df_prop['weekday'].replace(10, 10)

        test_df_prop['hr'] = date.hour
        test_df_prop

        
        # training k fold using time series split
        tscv = TimeSeriesSplit(n_splits = 4)
        for train_index, test_index in tscv.split(dfx):
            cv_train, cv_test = dfx.iloc[train_index], dfx.iloc[test_index]
            # trainiing Holts winter on split of train set
            model = ExponentialSmoothing(cv_train,seasonal='add', seasonal_periods=672)
            results = model.fit()
            predictions = results.predict(cv_test.index.values[0], cv_test.index.values[-1])
            
            # storing predictions and goundtruth for Holts winter
            expreds = pd.concat([expreds, predictions], axis=0).reset_index(drop=True)
            exgt = pd.concat([exgt, cv_test], axis=0).reset_index(drop=True)
            true_values = cv_test.values

            print("   ", test_index)
            rmse=np.sqrt(metrics.mean_squared_error(true_values, predictions))
            print("   " , rmse)
            
            cv_train, cv_test = df_prop.iloc[train_index, :], df_prop.iloc[test_index, :]
            train_df = cv_train
            test_df = cv_test
            # training predictions and goundtruth for Prophet
            m=Prophet(growth='linear',yearly_seasonality=False,daily_seasonality=True,seasonality_mode='additive',seasonality_prior_scale=5).add_seasonality(name='custom',period=1,fourier_order=5)
            m.add_regressor('weekday')
            m.add_regressor('hr')
            m.fit(train_df);
            

            future = test_df.drop('y', axis=1)
            future['ds'] = pd.to_datetime(future['ds'])
            forecast = m.predict(future)
            
            propreds = pd.concat([propreds, forecast['yhat']], axis=0).reset_index(drop=True)
            progt = pd.concat([progt, cv_test], axis=0).reset_index(drop=True)

            
        # training xgbregressor with predictions of base models and groundth values
        xgb = XGBRegressor()
        xgtrain = pd.concat([expreds, propreds], axis=1)
        xgtrain.columns = ['ex', 'prop']
        xgb.fit(xgtrain, exgt)
        
        
        # getting predictions for test with base models
        
        model = ExponentialSmoothing(dfx,seasonal='add', seasonal_periods=672)
        results = model.fit()
        expredictions = pd.DataFrame(results.predict(dfx.index[-1]+1, dfx.index[-1]+len(df_test)))
        expredictions.index = range(len(expredictions))

        
        m=Prophet(growth='linear',yearly_seasonality=False,daily_seasonality=True,seasonality_mode='additive',seasonality_prior_scale=96).add_seasonality(name='custom',period=1,fourier_order=24)
        # adding regressor variables
        m.add_regressor('weekday')
        m.add_regressor('hr')
        
        m.fit(df_prop);


        future = test_df_prop.drop('y', axis=1)
        future['ds'] = pd.to_datetime(future['ds'])
        forecast = m.predict(future)
        forecast['yhat']
        
        # getting predictions for test xgbregressor
        
        
        xgval = pd.concat([expredictions, pd.Series(forecast['yhat'])], axis=1)
        xgval.columns = ['ex', 'prop']

        
        xg_preds = xgb.predict(xgval)
        for_test.loc[df_test.index, meter] = xg_preds
        print("evals is", myeval(xg_preds, data.loc[df_test.index, meter]))
        ev+=myeval(xg_preds, data.loc[df_test.index, meter])
        break
    break
    evals.append(ev/3)



# basically repeating the values by putting split of 20000 and 6400 to whole dataset and test file 



df = pd.read_csv('../input/gc/train.csv')
for_test = pd.read_csv('../input/gc/test.csv')
data = pd.read_csv('../input/gc/train.csv')
buildings = [1, 2, 3, 4, 5]
meters = ['main_meter', 'sub_meter_1', 'sub_meter_2']
for_test['main_meter'] = 0
for_test['sub_meter_1'] = 0
for_test['sub_meter_2'] = 0
for_test.loc[:, meters]=0

for building in buildings:
    for meter in meters:
        #selecting building and meter
        
        print("building:")
        print(building)
        print("meter")
        print(meter)
        print("the value of rmse is")
        expreds = pd.DataFrame(columns=[])
        exgt = pd.DataFrame(columns=[])
        propreds = pd.DataFrame(columns=[])
        progt = pd.DataFrame(columns=[])
        dfx = data[data['building_number']==building][meter]
        df_test = for_test[for_test['building_number']==building][meter]
        
        df_prop = data[data['building_number']==building]
        df_prop = df_prop.loc[:, ['timestamp', meter]]
        df_prop.columns = ['ds', 'y']
        
        test_df_prop = for_test[for_test['building_number']==building]
        test_df_prop = test_df_prop.loc[:, ['timestamp', meter]]
        test_df_prop.columns = ['ds', 'y']
        
        # creating regressors
        
        date = pd.DatetimeIndex(df_prop['ds'])
        date.weekday
        df_prop['weekday'] = date.weekday
        # df_prop.drop('weekday', axis=1, inplace=True)
        df_prop['weekday'] = df_prop['weekday'].replace(0, 10)
        df_prop['weekday'] = df_prop['weekday'].replace(5, 0)
        df_prop['weekday'] = df_prop['weekday'].replace(4, 0)
        df_prop['weekday'] = df_prop['weekday'].replace(3, 0)
        df_prop['weekday'] = df_prop['weekday'].replace(2, 0)
        df_prop['weekday'] = df_prop['weekday'].replace(1, 0)
        df_prop['weekday'] = df_prop['weekday'].replace(6, 0)
        df_prop['weekday'] = df_prop['weekday'].replace(10, 10)

        df_prop['hr'] = date.hour
        df_prop

        date = pd.DatetimeIndex(test_df_prop['ds'])
        date.weekday
        test_df_prop['weekday'] = date.weekday
        # test_df_prop.drop('weekday', axis=1, inplace=True)
        test_df_prop['weekday'] = test_df_prop['weekday'].replace(0, 10)
        test_df_prop['weekday'] = test_df_prop['weekday'].replace(5, 0)
        test_df_prop['weekday'] = test_df_prop['weekday'].replace(4, 0)
        test_df_prop['weekday'] = test_df_prop['weekday'].replace(3, 0)
        test_df_prop['weekday'] = test_df_prop['weekday'].replace(2, 0)
        test_df_prop['weekday'] = test_df_prop['weekday'].replace(1, 0)
        test_df_prop['weekday'] = test_df_prop['weekday'].replace(6, 0)
        test_df_prop['weekday'] = test_df_prop['weekday'].replace(10, 10)

        test_df_prop['hr'] = date.hour
        test_df_prop

        
        
        
        
        # training k fold using time series split
        
        tscv = TimeSeriesSplit(n_splits = 4)
        for train_index, test_index in tscv.split(dfx):
            cv_train, cv_test = dfx.iloc[train_index], dfx.iloc[test_index]
            # trainiing Holts winter on split of train set
            
            model = ExponentialSmoothing(cv_train,seasonal='add', seasonal_periods=672)
            results = model.fit()
            predictions = results.predict(cv_test.index.values[0], cv_test.index.values[-1])
            # storing predictions and goundtruth for Holts winter
            
            expreds = pd.concat([expreds, predictions], axis=0).reset_index(drop=True)
            exgt = pd.concat([exgt, cv_test], axis=0).reset_index(drop=True)
            true_values = cv_test.values
    #         truevalues.append(true_values)
            print("   ", test_index)
            rmse=np.sqrt(metrics.mean_squared_error(true_values, predictions))
            print("   " , rmse)
            
            # training predictions and goundtruth for Prophet
            
            cv_train, cv_test = df_prop.iloc[train_index, :], df_prop.iloc[test_index, :]
            train_df = cv_train
            test_df = cv_test
            m=Prophet(growth='linear',yearly_seasonality=False,daily_seasonality=True,seasonality_mode='additive',seasonality_prior_scale=96).add_seasonality(name='custom',period=1,fourier_order=24)
            m.add_regressor('weekday')
            m.add_regressor('hr')
            m.fit(train_df);
            
            future = test_df.drop('y', axis=1)
            future['ds'] = pd.to_datetime(future['ds'])
            forecast = m.predict(future)
            propreds = pd.concat([propreds, forecast['yhat']], axis=0).reset_index(drop=True)
            progt = pd.concat([progt, cv_test], axis=0).reset_index(drop=True)
        # training xgbregressor with predictions of base models and groundth values
        
        xgb = XGBRegressor()
        xgtrain = pd.concat([expreds, propreds], axis=1)
        xgtrain.columns = ['ex', 'prop']
        xgb.fit(xgtrain, exgt)
        # getting predictions for test with base models
        
        model = ExponentialSmoothing(dfx,seasonal='add', seasonal_periods=672)
        results = model.fit()
        expredictions = pd.DataFrame(results.predict(dfx.index[-1]+1, dfx.index[-1]+len(df_test)))
        expredictions.index = range(len(expredictions))

        
        m=Prophet(growth='linear',yearly_seasonality=False,daily_seasonality=True,seasonality_mode='additive',seasonality_prior_scale=96).add_seasonality(name='custom',period=1,fourier_order=24)
        m.add_regressor('weekday')
        m.add_regressor('hr')
        
        m.fit(df_prop);
        future = test_df_prop.drop('y', axis=1)
        future['ds'] = pd.to_datetime(future['ds'])
        forecast = m.predict(future)
        forecast['yhat']
        
        # getting predictions for test xgbregressor
        
        xgval = pd.concat([expredictions, pd.Series(forecast['yhat'])], axis=1)
        xgval.columns = ['ex', 'prop']
        xg_preds = xgb.predict(xgval)
        for_test.loc[df_test.index, meter] = xg_preds
#         print("evals is", myeval(xg_preds, data.loc[df_test.index, meter]))

for_test

for_test.to_csv('test_predictions_commit.csv', index=False)